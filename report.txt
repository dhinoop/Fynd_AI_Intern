Task-1: Review Classification Using LLM Prompting
Objective

Predict a Yelp review’s star rating (1–5) using prompt-based LLM inference, returning strictly formatted JSON:

{
  "predicted_stars": <number>,
  "explanation": "<brief reasoning>"
}

Prompting Approaches Implemented

Zero-Shot Prompting
Direct instruction to classify without examples.

Few-Shot Prompting
Included sample labeled reviews to guide the model.

Hidden Chain-of-Thought (CoT)
Asked model to “think step-by-step but hide reasoning”, returning only JSON.

Implementation Journey & Decisions
Initial Approach: Google Gemini API

Multiple technical obstacles were encountered:

Free-tier quota exhausted quickly for 200+ samples

Model version errors (404 – model not found)

Token-limit mismatch (max_output_tokens deprecated)

Slow rate limits → 60s+ delays

JSON occasionally malformed

Conclusion: API-based approach was unstable and too slow for the dataset size.

Final Approach: Local LLaMA 3 (Ollama)

Switched to local inference for speed, reliability, and unlimited usage.

Key fixes implemented:

Removed ANSI spinner characters from Ollama’s streaming output

Robust JSON cleaning/extraction

Escaped {} braces inside f-strings

Sanitized multi-line text for CSV

Ensured all responses contained "predicted_stars" and "explanation"

Task-2: Two-Dashboard AI Feedback System
Objective

Build a fully functional web application with:

User Dashboard:
Users submit review + star rating → receive an AI-generated reply.

Admin Dashboard:
Shows all submissions live, including:

Rating

Review

AI Summary

AI Recommended Actions

Basic analytics (total reviews, average rating)

Both dashboards use the same JSON storage for consistency.

| Component  | Technology                      |
| ---------- | ------------------------------- |
| Backend    | Python Flask + SSE              |
| LLM        | **Groq LLaMA 3.1 (8B Instant)** |
| Storage    | JSON file                       |
| Deployment | Docker + Render Web Service     |
| Frontend   | HTML, CSS, JavaScript           |


AI Generation Logic

User-facing Reply:
Short friendly message based on rating + review.

Admin Summary:
LLaMA produces a single-sentence summary.

Next Actions:
Recommends 2–3 improvements for the business.

Key Features

Server-Sent Events (SSE) for live updates on admin page

Password-protected admin URL (/admin?pwd=admin123)

Fully containerized (Dockerfile + requirements.txt)

Online deployment via Render

Robust fallback behavior if no API key is present

Latest supported Groq model used (groq/llama-3.1-8b-instant)
