Final Summary for Task-1 Report
Overview

The goal of Task-1 was to classify Yelp reviews into a 1–5 star rating using prompt-based approaches. Three prompting strategies were implemented:

Zero-Shot Prompting

Few-Shot Prompting

Hidden Chain-of-Thought (Hidden-CoT)

Each model response was required to return strictly formatted JSON:

{
  "predicted_stars": <number>,
  "explanation": "<brief reasoning>"
}

Implementation Notes
Attempted API-based LLMs (Gemini 1.5 & 2.0)

Initially, the system was built using Google Gemini API.

Faced significant obstacles:

Free-tier request quota exceeded frequently.

Model version mismatches (404 model not found).

Missing parameters (max_output_tokens deprecated).

API rate-limit delays (>60 seconds).

These issues made API-based experiments unreliable for a 200-sample dataset.

→ Decision: Switch entirely to local LLM inference using Ollama + LLaMA 3.

Switch to Local LLAMA3 (Ollama)
Why local LLM?

No rate limits.

Faster iteration cycle.

More control over output formatting.

Reproducible behavior.

Challenges solved:

ANSI spinner characters removed using regex.

JSON extraction improved with robust parsing.

Prompts adjusted with escaped braces {} to avoid formatting conflicts.

Needed to sanitize multiline text for CSV.

Ensured output contained "predicted_stars" and "explanation".

Sampling Strategy

Full dataset has >1M records → too slow for local inference.

Running 200 samples locally took too much time, as LLAMA3 needed ~8–12 seconds per review depending on prompt.

To meet time constraints, we processed 20 sampled reviews for final evaluation.

*********************************************************************************************************************************

Task 2 – Two-Dashboard AI Feedback System

(Using LLaMA via Ollama — Final Summary)**

1. Objective

The goal of Task-2 was to design, build, and deploy a complete AI-powered web feedback system with:

A public user dashboard for collecting ratings and text reviews.

An internal admin dashboard for viewing feedback, AI summaries, and recommended actions.

AI automation using a Large Language Model (LLaMA).

Shared data storage between both dashboards.

Full online deployment so both dashboards are accessible publicly.

2. Technology Stack
✔ Backend

Flask (Python)

Ollama (LLaMA 3) for local AI inference

Server-Sent Events (SSE) for real-time admin updates

Data stored in a JSON file (reviews.json)

✔ Frontend

HTML + CSS + Vanilla JavaScript

Two pages:

user.html

admin.html

✔ LLM Used

The application uses:

LLaMA 3 via Ollama (Local LLM Runtime)

All AI tasks are handled locally:

User-facing responses

Review summarization

Recommended next actions

This avoids rate-limits, ensures privacy, and removes cloud dependency.

3. Features Implemented
A. User Dashboard (Public-Facing)

Users can:

Select a rating (1–5)

Submit a written review

Receive an AI-generated reply created by LLaMA

Automatically store the review in reviews.json

B. Admin Dashboard (Internal-Facing)

Admins can:

Access dashboard via URL:
/admin?pwd=YOUR_PASSWORD

View all reviews with:

User rating

User’s review text

AI-generated summary

AI-suggested next actions

See live updates (new reviews appear instantly via SSE)

View analytics, including:

Rating distribution chart

Recent review trends

4. Data Storage

Both dashboards read/write from the same file:

data/reviews.json

5. Deployment

Both dashboards were deployed as a single Flask web app using:

✔ Render Web Service (Online Deployment)

This achieved:

Public URL for User dashboard

Public URL for Admin dashboard

Environment variables stored securely on Render

LLaMA running on the backend machine (Ollama installed)

Thus, both dashboards are fully live and accessible online, as required.

6. Final Outcome

Task-2 is successfully completed, delivering a:

✔ Fully functional, AI-driven feedback system
✔ User dashboard + Admin dashboard
✔ Local LLaMA-based AI pipeline
✔ Real-time updates
✔ Online deployment of both dashboards
